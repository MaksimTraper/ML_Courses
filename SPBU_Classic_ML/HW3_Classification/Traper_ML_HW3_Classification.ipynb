{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNa-Hf2lXyZW"
   },
   "source": [
    "# Задача 3. Сравнение методов классификации\n",
    "\n",
    "## Список задач\n",
    "1. Самостоятельно реализовать один из методов классификации, с возможностью настройки гиперпараметров.\n",
    "2. Взять данные для предсказания заболеваний сердца.\n",
    "3. Считать данные, выполнить первичный анализ данных, при необходимости произвести чистку данных (Data Cleaning).\n",
    "4. Выполнить разведочный анализ (EDA), использовать визуализацию, сделать выводы, которые могут быть полезны при дальнейшем решении задачи классификации.\n",
    "5. При необходимости выполнить полезные преобразования данных (например, трансформировать категариальные признаки в количественные), убрать ненужные признаки, создать новые (Feature Engineering).\n",
    "6. Используя подбор гиперпараметров, кросс-валидацию и при необходимости масштабирование данных, добиться наилучшего качества предсказания от Вашей реализации на выделенной заранее тестовой выборке.\n",
    "7. Повторить предыдущий пункт для библиотечных реализаций (например, из sklearn) всех пройденных методов классификации (logistic regression, svm, knn, naive bayes, decision tree).\n",
    "8. Сравнить все обученные модели, построить их confusion matrices. Сделать выводы о полученных моделях в рамках решения задачи классификации на выбранных данных.\n",
    "9. Реализовать еще один из методов классификации и добавить его в сравнение.\n",
    "10. Найти данные, на которых интересно будет решать задачу классификации. Повторить все пункты задания на новых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1/9. Самостоятельно реализовать два метода классификации, с возможностью настройки гиперпараметров.\n",
    "Реализую дерево решений (Decision Tree) по алгоритму CART и метод опорных векторов (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    \"\"\"\n",
    "    Класс метрик качества для задач классификации и кластеризации\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def gini(y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Вычисляем коэффициент Gini для вектора целевых переменных\n",
    "\n",
    "        Параметры:\n",
    "            y (np.ndarray): Одномерный массив (целочисленный вектор)\n",
    "\n",
    "        Возвращает:\n",
    "            float: Значение коэффициента Gini\n",
    "        \"\"\"\n",
    "        count_targets = len(y)\n",
    "        if count_targets == 0:\n",
    "            return 0.0\n",
    "\n",
    "        _, count_each_target = np.unique(y, return_counts=True)\n",
    "        probabilities = count_each_target / count_targets\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_split(left_y: np.ndarray, right_y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Вычисляем коэффициент Gini для узла при выбранном пороге\n",
    "\n",
    "        Параметры:\n",
    "            left_y: Массив целевых переменных для левой ветви.\n",
    "            right_y: Массив целевых переменных для правой ветви.\n",
    "\n",
    "        Возвращает:\n",
    "            float: Взвешенное значение Gini для разбиения.\n",
    "        \"\"\"\n",
    "        count_objects = len(left_y) + len(right_y)\n",
    "\n",
    "        left_leaf_loss = Metrics.gini(left_y)\n",
    "        right_leaf_loss = Metrics.gini(right_y)\n",
    "        weighted_gini = ((len(left_y) / count_objects) * left_leaf_loss) + \\\n",
    "                        ((len(right_y) / count_objects) * right_leaf_loss)\n",
    "\n",
    "        return weighted_gini, left_leaf_loss, right_leaf_loss\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Класс узла дерева\n",
    "\n",
    "    Параметры:\n",
    "        X: подвыборка признаков в узле\n",
    "        y: подвыборка целевых переменных в узле\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X: pl.DataFrame = None, y: np.ndarray = None, depth: int = 0):\n",
    "        self.left_node = None\n",
    "        self.right_node = None\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        self.feature = None\n",
    "        self.threshold = None\n",
    "\n",
    "        self.score = Metrics.gini(y) if y is not None else None\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "    def create_child_nodes(self, metric='Gini', max_depth=3, min_samples_split=2):\n",
    "        \"\"\"\n",
    "        Функция создания дочерних узлов, присвоение им выборки соответствующих целевых переменных\n",
    "\n",
    "        Параметры:\n",
    "            metric: выбранная метрика качества\n",
    "            max_depth: максимальная глубина дерева\n",
    "            min_samples_split: минимальное количество образцов для разделения\n",
    "\n",
    "        Возвращает:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self.depth >= max_depth or len(self.y) < min_samples_split or self.score == 0:\n",
    "            return\n",
    "\n",
    "        best_feature, best_threshold, best_score, left_indices, right_indices = self.__choose_best_split(metric)\n",
    "        if best_feature is None:\n",
    "            return\n",
    "\n",
    "        self.feature = best_feature\n",
    "        self.threshold = best_threshold\n",
    "\n",
    "        X_left = self.X[left_indices]\n",
    "        y_left = self.y[left_indices]\n",
    "        X_right = self.X[right_indices]\n",
    "        y_right = self.y[right_indices]\n",
    "\n",
    "        self.left_node = Node(X_left, y_left, depth=self.depth + 1)\n",
    "        self.right_node = Node(X_right, y_right, depth=self.depth + 1)\n",
    "\n",
    "        self.left_node.create_child_nodes(metric, max_depth, min_samples_split)\n",
    "        self.right_node.create_child_nodes(metric, max_depth, min_samples_split)\n",
    "\n",
    "    def __choose_best_split(self, metric='Gini'):\n",
    "        \"\"\"\n",
    "        Вычисляет оптимальный признак и порог для узла, исходя из выбранной метрики\n",
    "\n",
    "        Возвращает:\n",
    "            best_feature, best_threshold, best_score, left_indices, right_indices\n",
    "        \"\"\"\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_score = np.inf\n",
    "        best_left_indices = None\n",
    "        best_right_indices = None\n",
    "\n",
    "        features = self.X.columns\n",
    "\n",
    "        for feature in features:\n",
    "            X_column = self.X[feature].to_numpy()\n",
    "            thresholds, scores, left_indices_list, right_indices_list = self.__find_thresholds(X_column, self.y, metric)\n",
    "\n",
    "            if len(scores) == 0:\n",
    "                continue\n",
    "\n",
    "            min_score_index = np.argmin(scores)\n",
    "            score = scores[min_score_index]\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_threshold = thresholds[min_score_index]\n",
    "                best_feature = feature\n",
    "                best_left_indices = left_indices_list[min_score_index]\n",
    "                best_right_indices = right_indices_list[min_score_index]\n",
    "\n",
    "        return best_feature, best_threshold, best_score, best_left_indices, best_right_indices\n",
    "\n",
    "    def __find_thresholds(self, X_column: np.ndarray, y: np.ndarray, metric='Gini'):\n",
    "        \"\"\"\n",
    "        Находит все возможные пороги для признака и вычисляет метрику для каждого\n",
    "\n",
    "        Возвращает:\n",
    "            thresholds, scores, left_indices_list, right_indices_list\n",
    "        \"\"\"\n",
    "        thresholds = []\n",
    "        scores = []\n",
    "        left_indices_list = []\n",
    "        right_indices_list = []\n",
    "\n",
    "        sorted_indices = np.argsort(X_column)\n",
    "        X_sorted = X_column[sorted_indices]\n",
    "        y_sorted = y[sorted_indices]\n",
    "\n",
    "        unique_values = np.unique(X_sorted)\n",
    "        if len(unique_values) == 1:\n",
    "            return thresholds, scores, left_indices_list, right_indices_list\n",
    "\n",
    "        for i in range(1, len(X_sorted)):\n",
    "            if X_sorted[i] == X_sorted[i - 1]:\n",
    "                continue  # пропускаем повторяющиеся значения\n",
    "\n",
    "            threshold = (X_sorted[i] + X_sorted[i - 1]) / 2\n",
    "\n",
    "            left_indices = sorted_indices[:i]\n",
    "            right_indices = sorted_indices[i:]\n",
    "\n",
    "            left_y = y[left_indices]\n",
    "            right_y = y[right_indices]\n",
    "\n",
    "            if metric == 'Gini':\n",
    "                score, _, _ = Metrics.gini_split(left_y, right_y)\n",
    "            else:\n",
    "                raise ValueError(f\"Неизвестная метрика: {metric}\")\n",
    "\n",
    "            thresholds.append(threshold)\n",
    "            scores.append(score)\n",
    "            left_indices_list.append(left_indices)\n",
    "            right_indices_list.append(right_indices)\n",
    "\n",
    "        return thresholds, scores, left_indices_list, right_indices_list\n",
    "\n",
    "    def majority_class(self):\n",
    "        \"\"\"\n",
    "        Возвращает класс большинства в листе.\n",
    "        \"\"\"\n",
    "        values, counts = np.unique(self.y, return_counts=True)\n",
    "        return values[np.argmax(counts)]\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Класс решающего дерева, реализованного по алгоритму CART\n",
    "\n",
    "    Параметры:\n",
    "        max_depth: максимальная глубина дерева\n",
    "        metric: метрика качества\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth: int = 3, metric: str = 'Gini', min_samples_split: int = 2):\n",
    "        self.max_depth = max_depth\n",
    "        self.metric = metric\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "    \n",
    "    def encode_categorical_features(self, X: pl.DataFrame):\n",
    "        \"\"\"\n",
    "        Метод кодирования категориальных переменных с помощью One-Hot Encoding\n",
    "\n",
    "        Параметры:\n",
    "            X: датафрейм данных\n",
    "\n",
    "        Возвращает:\n",
    "            pl.DataFrame: датафрейм данных с закодированными категориальными переменными\n",
    "        \"\"\"\n",
    "        categorical_features = [feature for feature, dtype in X.schema.items() if dtype == pl.Utf8]\n",
    "        if len(categorical_features) == 0:\n",
    "            return X\n",
    "\n",
    "        X_encoded = X.to_pandas()\n",
    "        X_encoded = pd.get_dummies(X_encoded, columns=categorical_features)\n",
    "        return pl.from_pandas(X_encoded)\n",
    "    \n",
    "    def fit(self, X: Union[np.ndarray, pd.DataFrame, pl.DataFrame], y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Метод построения решающего дерева на основе данных\n",
    "\n",
    "        Параметры:\n",
    "            X: датафрейм данных\n",
    "            y: вектор целевых переменных\n",
    "\n",
    "        Возвращает:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # Преобразование данных в pl.DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = pl.from_pandas(X)\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            X = pl.DataFrame(X)\n",
    "        elif not isinstance(X, pl.DataFrame):\n",
    "            raise ValueError(\"X должен быть pd.DataFrame, pl.DataFrame или np.ndarray\")\n",
    "\n",
    "        # Кодирование категориальных признаков\n",
    "        X = self.encode_categorical_features(X)\n",
    "\n",
    "        self.features = X.columns\n",
    "\n",
    "        self.root = Node(X, y, depth=0)\n",
    "        self.root.create_child_nodes(metric=self.metric, max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: Union[np.ndarray, pd.DataFrame, pl.DataFrame]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Метод предсказания целевой метки объектов\n",
    "\n",
    "        Параметры:\n",
    "            X: датафрейм данных\n",
    "\n",
    "        Возвращает:\n",
    "            np.ndarray: список предсказанных целевых меток\n",
    "        \"\"\"\n",
    "        # Преобразование данных в pl.DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = pl.from_pandas(X)\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            X = pl.DataFrame(X, columns=self.features)\n",
    "        elif not isinstance(X, pl.DataFrame):\n",
    "            raise ValueError(\"X должен быть pd.DataFrame, pl.DataFrame или np.ndarray\")\n",
    "\n",
    "        # Кодирование категориальных признаков\n",
    "        X = self.encode_categorical_features(X)\n",
    "\n",
    "        y_pred = []\n",
    "        \n",
    "        for row_idx in range(X.height):\n",
    "            current_node = self.root\n",
    "            while current_node.left_node is not None and current_node.right_node is not None:\n",
    "                if current_node.feature not in X.columns:\n",
    "                    break  # если признак отсутствует после кодирования\n",
    "                value = X[current_node.feature][row_idx]\n",
    "                if value <= current_node.threshold:\n",
    "                    current_node = current_node.left_node\n",
    "                else:\n",
    "                    current_node = current_node.right_node\n",
    "            y_pred.append(current_node.majority_class())\n",
    "    \n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def pruning(self, X_val: Union[np.ndarray, pd.DataFrame, pl.DataFrame], y_val: np.ndarray, alpha: float = 0.0):\n",
    "        \"\"\"\n",
    "        Обрезка дерева (pruning) с использованием метода минимальной стоимости сложения (Cost-Complexity Pruning)\n",
    "\n",
    "        Параметры:\n",
    "            X_val: валидационные данные\n",
    "            y_val: целевые метки для валидационных данных\n",
    "            alpha (float): параметр сложности модели\n",
    "\n",
    "        Возвращает:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # Реализация прунинга на основе минимизации функционала с параметром alpha\n",
    "        # Эта часть может быть довольно сложной и требует полноценной реализации алгоритма CCP\n",
    "        # Ниже приведен упрощенный пример\n",
    "\n",
    "        # Вычисляем предсказания на валидационном наборе до прунинга\n",
    "        y_pred = self.predict(X_val)\n",
    "        best_accuracy = np.mean(y_pred == y_val)\n",
    "        best_tree = self.root\n",
    "\n",
    "        def prune_node(node: Node):\n",
    "            if node.left_node is None or node.right_node is None:\n",
    "                return\n",
    "\n",
    "            # Сохраняем ссылки на дочерние узлы\n",
    "            left_backup = node.left_node\n",
    "            right_backup = node.right_node\n",
    "\n",
    "            # Обрезаем узлы\n",
    "            node.left_node = None\n",
    "            node.right_node = None\n",
    "\n",
    "            # Проверяем качество на валидационной выборке\n",
    "            y_pred = self.predict(X_val)\n",
    "            accuracy = np.mean(y_pred == y_val)\n",
    "\n",
    "            # Если качество улучшилось или не изменилось, оставляем узел обрезанным\n",
    "            if accuracy >= best_accuracy - alpha:\n",
    "                nonlocal best_accuracy, best_tree\n",
    "                best_accuracy = accuracy\n",
    "                best_tree = self.root\n",
    "            else:\n",
    "                # Возвращаем дочерние узлы обратно\n",
    "                node.left_node = left_backup\n",
    "                node.right_node = right_backup\n",
    "\n",
    "                # Рекурсивно пытаемся обрезать дочерние узлы\n",
    "                prune_node(node.left_node)\n",
    "                prune_node(node.right_node)\n",
    "\n",
    "        prune_node(self.root)\n",
    "        self.root = best_tree\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \"\"\"\n",
    "    Класс метода опорных векторов, линейно разделяющего данные\n",
    "\n",
    "    Параметры:\n",
    "        X: датасет признаков\n",
    "        y: вектор целевых меток\n",
    "        С: коэффициент регуляризации\n",
    "    \"\"\"\n",
    "    def __init__(self, X: Union[np.ndarray, pd.DataFrame, pl.DataFrame], \n",
    "                 y: Union[np.ndarray, pd.DataFrame, pl.DataFrame],\n",
    "                 C: float = 0.0):\n",
    "        if isinstance(X, (pl.DataFrame, pd.DataFrame)):\n",
    "            X = X.to_numpy()\n",
    "        self.X = X\n",
    "\n",
    "        if isinstance(y, (pl.DataFrame, pd.DataFrame)):\n",
    "            y = y.to_numpy()\n",
    "        self.y = y\n",
    "    \n",
    "        self.count_objects = self.X.shape[0]\n",
    "        self.count_features = self.X.shape[1]\n",
    "\n",
    "        self.C = C\n",
    "        self.W = np.zeros(self.count_features)\n",
    "        self.b = 0.0\n",
    "\n",
    "    def __gradient_descent(self, threshold: float = 1e-3, lr: float = 1e-2, max_iter: int = 10000):\n",
    "        \"\"\"\n",
    "        Подбор параметров линейно разделяющего SVM методом градиентного спуска\n",
    "\n",
    "        Параметры:\n",
    "            threshold: порог, если меньше которого будет изменение, то алгоритм остановится\n",
    "            lr: скорость обучения\n",
    "            max_iter: максимальное количество итераций\n",
    "\n",
    "        Возвращает:\n",
    "            self\n",
    "        \"\"\"\n",
    "        for step in range(max_iter):\n",
    "            hinge_loss_gradient = 1 - self.y * (self.W.dot(self.X.T) + self.b)\n",
    "            mask = hinge_loss_gradient > 0\n",
    "\n",
    "            dW = self.W - self.C * np.sum((self.y[mask] * self.X[mask]), axis=0)\n",
    "            db = -self.C * np.sum(self.y[mask])\n",
    "\n",
    "            self.W -= lr * dW\n",
    "            self.b -= lr * db\n",
    "            \n",
    "            # Условие выхода\n",
    "            if np.linalg.norm(dW) < threshold and abs(db) < threshold:\n",
    "                break\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Обучение линейно разделяющего классификатора SVM\n",
    "\n",
    "        Возвращает:\n",
    "            self\n",
    "        \"\"\"\n",
    "        self.__gradient_descent()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: Union[np.ndarray, pd.DataFrame, pl.DataFrame]):\n",
    "        \"\"\"\n",
    "        Предсказание класса объекта\n",
    "\n",
    "        Параметры:\n",
    "            X: датафрейм объектов\n",
    "\n",
    "        Возвращает:\n",
    "            np.array: массив предсказанных целевых меток\n",
    "        \"\"\"\n",
    "        if isinstance(X, (pl.DataFrame, pd.DataFrame)):\n",
    "            X = X.to_numpy()\n",
    "\n",
    "        if self.W is None or self.b is None:\n",
    "            raise ValueError(\"Модель ещё не обучена. Сначала вызовите метод `fit`.\")\n",
    "\n",
    "        decision = np.dot(X, self.W) + self.b\n",
    "        # Применяем функцию знака\n",
    "        return np.sign(decision)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yz3-pAdOZBYh",
    "pW4msAz2j0FG",
    "Frb4W3BUo7py",
    "rJ5xYIj4xfs6",
    "_M0TM2Dm0sGi",
    "rIPBXJVp1yVn",
    "xCGM34iX170T",
    "2mT-Aa5s2CYI"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
